#!/usr/bin/env python3
"""
Script pour exÃ©cuter la pipeline Big Data complÃ¨te manuellement

Ce script remplace Airflow temporairement et exÃ©cute toutes les Ã©tapes :
1. GÃ©nÃ©ration des donnÃ©es
2. Ingestion Kafka
3. Stockage HDFS
4. Traitement Spark
5. Analytics
6. Export MySQL
7. Rapports
"""

import subprocess
import sys
import os
import time
import logging
from datetime import datetime

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline_execution.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def run_command(command, description, timeout=300):
    """ExÃ©cute une commande avec timeout et logging"""
    logger.info(f"ğŸš€ DÃ©marrage: {description}")
    logger.info(f"Commande: {command}")

    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=timeout
        )

        if result.returncode == 0:
            logger.info(f"âœ… SuccÃ¨s: {description}")
            if result.stdout:
                logger.info(f"Output: {result.stdout[:500]}...")
            return True, result.stdout
        else:
            logger.error(f"âŒ Ã‰chec: {description}")
            logger.error(f"Erreur: {result.stderr}")
            return False, result.stderr

    except subprocess.TimeoutExpired:
        logger.error(f"â° Timeout: {description}")
        return False, "Timeout"
    except Exception as e:
        logger.error(f"ğŸ’¥ Exception: {description} - {e}")
        return False, str(e)

def step_1_generate_data():
    """Ã‰tape 1: GÃ©nÃ©ration des donnÃ©es"""
    logger.info("ğŸ”„ Ã‰TAPE 1: GÃ‰NÃ‰RATION DES DONNÃ‰ES")

    # CrÃ©er le rÃ©pertoire data
    os.makedirs('data', exist_ok=True)

    # GÃ©nÃ©rer les donnÃ©es
    success, output = run_command(
        "python3 traffic_data_generator.py",
        "GÃ©nÃ©ration des Ã©vÃ©nements de trafic",
        timeout=60
    )

    if success:
        # VÃ©rifier que le fichier a Ã©tÃ© crÃ©Ã©
        if os.path.exists('traffic_events.json'):
            with open('traffic_events.json', 'r') as f:
                lines = sum(1 for _ in f)
            logger.info(f"ğŸ“Š {lines} Ã©vÃ©nements gÃ©nÃ©rÃ©s")
            return True
        else:
            logger.error("Fichier traffic_events.json non trouvÃ©")
            return False

    return success

def step_2_kafka_ingestion():
    """Ã‰tape 2: Ingestion Kafka"""
    logger.info("ğŸ”„ Ã‰TAPE 2: INGESTION KAFKA")

    success, output = run_command(
        "python3 kafka-producer.py",
        "Ingestion des donnÃ©es dans Kafka",
        timeout=120
    )

    return success

def step_3_hdfs_storage():
    """Ã‰tape 3: Stockage HDFS"""
    logger.info("ğŸ”„ Ã‰TAPE 3: STOCKAGE HDFS")

    # Utiliser le script wrapper pour HDFS
    success, output = run_command(
        "python3 scripts/run_pipeline_step.py create_hdfs_dirs $(date +%Y-%m-%d)",
        "CrÃ©ation des rÃ©pertoires HDFS",
        timeout=60
    )

    if not success:
        return False

    # Ingestion vers HDFS
    success, output = run_command(
        "python3 kafka-consumer.py --topic traffic-events --bootstrap kafka:9092 --hdfs-path /data/raw/traffic/$(date +%Y-%m-%d)/events.json --max-messages 1000",
        "Stockage des donnÃ©es dans HDFS",
        timeout=180
    )

    if success:
        # VÃ©rification
        run_command(
            "python3 scripts/run_pipeline_step.py verify_hdfs $(date +%Y-%m-%d)",
            "VÃ©rification du stockage HDFS",
            timeout=30
        )

    return success

def step_4_spark_processing():
    """Ã‰tape 4: Traitement Spark"""
    logger.info("ğŸ”„ Ã‰TAPE 4: TRAITEMENT SPARK")

    success, output = run_command(
        "python3 scripts/run_pipeline_step.py spark_processing",
        "Traitement analytique avec Spark",
        timeout=300
    )

    return success

def step_5_analytics():
    """Ã‰tape 5: Zone Analytics"""
    logger.info("ğŸ”„ Ã‰TAPE 5: ZONE ANALYTICS")

    success, output = run_command(
        "python3 scripts/run_pipeline_step.py spark_analytics",
        "CrÃ©ation de la zone analytics",
        timeout=300
    )

    return success

def step_6_mysql_export():
    """Ã‰tape 6: Export MySQL"""
    logger.info("ğŸ”„ Ã‰TAPE 6: EXPORT MYSQL")

    success, output = run_command(
        "python3 scripts/run_pipeline_step.py spark_export",
        "Export des rÃ©sultats vers MySQL",
        timeout=300
    )

    if success:
        # VÃ©rification
        run_command(
            "python3 scripts/run_pipeline_step.py verify_mysql",
            "VÃ©rification de l'export MySQL",
            timeout=30
        )

    return success

def step_7_reports():
    """Ã‰tape 7: GÃ©nÃ©ration de rapports"""
    logger.info("ğŸ”„ Ã‰TAPE 7: RAPPORTS")

    # Calcul des KPIs
    run_command(
        "python3 scripts/calculate_kpis_etape6.py",
        "Calcul des KPIs",
        timeout=60
    )

    # GÃ©nÃ©ration du rapport
    run_command(
        "python3 scripts/visualization/generate_reports.py --date $(date +%Y-%m-%d) --output reports/traffic_report_$(date +%Y-%m-%d).pdf",
        "GÃ©nÃ©ration du rapport PDF",
        timeout=60
    )

    return True

def main():
    """Fonction principale"""
    print("ğŸš€ PIPELINE BIG DATA - EXÃ‰CUTION MANUELLE")
    print("=" * 50)
    print(f"DÃ©but: {datetime.now()}")
    print("=" * 50)

    steps = [
        ("GÃ©nÃ©ration des donnÃ©es", step_1_generate_data),
        ("Ingestion Kafka", step_2_kafka_ingestion),
        ("Stockage HDFS", step_3_hdfs_storage),
        ("Traitement Spark", step_4_spark_processing),
        ("Zone Analytics", step_5_analytics),
        ("Export MySQL", step_6_mysql_export),
        ("Rapports", step_7_reports)
    ]

    results = []

    for step_name, step_function in steps:
        print(f"\nâ–¶ï¸  {step_name}")
        print("-" * 30)

        start_time = time.time()
        success = step_function()
        end_time = time.time()

        duration = end_time - start_time
        status = "âœ… RÃ‰USSI" if success else "âŒ Ã‰CHEC"

        print(".1f")
        results.append((step_name, success, duration))

        if not success:
            logger.error(f"Pipeline arrÃªtÃ©e Ã  l'Ã©tape: {step_name}")
            break

        # Petite pause entre les Ã©tapes
        time.sleep(2)

    # RÃ©sumÃ© final
    print("\n" + "=" * 50)
    print("ğŸ“Š RÃ‰SUMÃ‰ DE L'EXÃ‰CUTION")
    print("=" * 50)

    total_time = sum(duration for _, _, duration in results)
    successful_steps = sum(1 for _, success, _ in results if success)

    for step_name, success, duration in results:
        status_icon = "âœ…" if success else "âŒ"
        print("5.1f")

    print(f"\nğŸ¯ RÃ©sultat: {successful_steps}/{len(steps)} Ã©tapes rÃ©ussies")
    print(".1f")

    if successful_steps == len(steps):
        print("\nğŸ‰ PIPELINE COMPLÃˆTE AVEC SUCCÃˆS !")
        print("ğŸ“Š Rendez-vous sur http://localhost:3000 pour voir les rÃ©sultats")
        print("ğŸ—ºï¸ La Heat Map gÃ©ographique est maintenant disponible !")
    else:
        print(f"\nâš ï¸ Pipeline partiellement exÃ©cutÃ©e ({successful_steps}/{len(steps)} Ã©tapes)")
        print("VÃ©rifiez les logs pour diagnostiquer les problÃ¨mes")

    print(f"\nFin: {datetime.now()}")

if __name__ == "__main__":
    main()
